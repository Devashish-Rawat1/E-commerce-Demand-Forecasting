import numpy as np
import pandas as pd
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from sklearn.model_selection import RandomizedSearchCV





df = pd.read_csv("../data/processed/featured_data.csv", parse_dates=["date"])


df.dtypes


df.head()





train_df = df[df["date"] < "2017-01-01"]
val_df   = df[df["date"] >= "2017-01-01"]


train_df.info()


val_df.info()


train_df.tail()


val_df.head()








TARGET = "sales"

DROP_COLS = ["date", "sales"]

FEATURES = [c for c in train_df.columns if c not in DROP_COLS]

X_train = train_df[FEATURES]
y_train = train_df[TARGET]

X_val = val_df[FEATURES]
y_val = val_df[TARGET]



FEATURES


## Training XGBoost model

xgb_model = XGBRegressor(
    n_estimators=500,
    max_depth=8,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    objective="reg:squarederror",
    random_state=42
)

xgb_model.fit(X_train, y_train)



val_preds = xgb_model.predict(X_val)
val_preds






rmse = np.sqrt(mean_squared_error(y_val, val_preds))
print("RMSE:", rmse)



#Business Metric
mape = np.mean(np.abs((y_val - val_preds) / y_val)) * 100
print("MAPE:", mape)






importance = pd.Series(
    xgb_model.feature_importances_,
    index=FEATURES
).sort_values(ascending=True)

plt.figure(figsize=(8,6))
importance.tail(15).plot(kind="barh")
plt.title("Top Feature Importances - XGBoost")
plt.savefig("../plots/xgboost_plots/Xgboost_feature_imp_val_pred.png")
plt.show()








plt.figure(figsize=(12,4))
plt.plot(y_val.values[:200], label="Actual")
plt.plot(val_preds[:200], label="Predicted")
plt.legend()
plt.title("Demand Forecast vs Actual (XGBoost-Validation_pred)")
plt.savefig("../plots/xgboost_plots/Xgboost_demand_vs_val_pred.png")
plt.show()






xgb = XGBRegressor(
    objective="reg:squarederror",
    random_state=42
)

param_grid = {
    "n_estimators": [300, 500, 800, 1000],
    "max_depth": [4, 6, 8, 10],
    "learning_rate": [0.01, 0.03, 0.05, 0.1],
    "subsample": [0.6, 0.8, 1.0],
    "colsample_bytree": [0.6, 0.8, 1.0],
    "min_child_weight": [1, 3, 5, 7],
    "gamma": [0, 0.1, 0.3, 0.5]
}

search = RandomizedSearchCV(
    xgb,
    param_distributions=param_grid,
    n_iter=25,
    scoring="neg_mean_absolute_error",
    cv=3,
    verbose=1,
    n_jobs=-1
)

search.fit(X_train, y_train)

best_model = search.best_estimator_

print("Best params:", search.best_params_)






val_preds_tuned = best_model.predict(X_val)

rmse_tuned = np.sqrt(mean_squared_error(y_val, val_preds_tuned))
mape_tuned = np.mean(np.abs((y_val - val_preds_tuned) / y_val)) * 100

print("Tuned RMSE:", rmse_tuned)
print("Tuned MAPE:", mape_tuned)











final_X = pd.concat([X_train, X_val])
final_y = pd.concat([y_train, y_val])

final_model = XGBRegressor(
    n_estimators=500,
    max_depth=8,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    objective="reg:squarederror",
    random_state=42
)

final_model.fit(final_X, final_y)




